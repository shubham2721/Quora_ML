{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.4 Featurising TF-IDF and AVG W2V</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# This package is used for finding longest common subsequence between two strings\n",
    "# you can write your own dp code for this\n",
    "from bs4 import BeautifulSoup\n",
    "from thefuzz import fuzz\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import final_inference as fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dataset\n",
    "abspath = os.getcwd()\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(os.path.join(abspath, 'Datasets/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before applying we can preprocess the data\n",
    "def pre_process(data):\n",
    "    x = str(data).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "\n",
    "    # Replacing Date with date string\n",
    "    pattern = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'\n",
    "    x = re.sub(pattern, 'date', x)\n",
    "\n",
    "    # Replacing links with link string\n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    x = re.sub(pattern, 'link', x)\n",
    "\n",
    "    #million and thousand representation\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "\n",
    "    # Stemming\n",
    "    porter = PorterStemmer()\n",
    "    # Removing special chars\n",
    "    pattern = re.compile('\\W')\n",
    "\n",
    "    if type(x) == type(''):\n",
    "        x = re.sub(pattern, ' ', x)\n",
    "\n",
    "    if type(x) == type(''):\n",
    "        x = porter.stem(x)\n",
    "        example1 = BeautifulSoup(x)\n",
    "        x = example1.get_text()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocess function over description data\n",
    "df['question1'] = df['question1'].apply(pre_process)\n",
    "df['question2'] = df['question2'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the questions\n",
    "questions = df['question1'].values + df['question2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dimensions:  7333\n"
     ]
    }
   ],
   "source": [
    "# creating TFIDF\n",
    "tf_idf = TfidfVectorizer(min_df = 50)\n",
    "tf_idf.fit_transform(questions)\n",
    "print('Total Dimensions: ', len(tf_idf.get_feature_names_out()))\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tf_idf.get_feature_names_out(), tf_idf.idf_))\n",
    "pickle.dump(tf_idf, open('trained_model/tf_idf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained model to obtain the w2v\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [1:04:12<00:00, 104.93it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Now for each row we need w2v \n",
    "# creating word to vec for question2\n",
    "q1_w2v = [] # this for each row\n",
    "\n",
    "#Step1. We need to loop thrrough each rows sentence\n",
    "for sentence in tqdm(df['question1'].values):\n",
    "    word_vec1 =  nlp(sentence)\n",
    "    # for each word we need to have empty vectors of the dimension (1, number of dimension trained)\n",
    "    word_vec = np.zeros(word_vec1.vector.shape)\n",
    "    word_cnt = 0\n",
    "    # Step:2 Looping through each sentence words\n",
    "    for word in word_vec1:\n",
    "        # Obtaining the polarity and numerical vector for each word\n",
    "        vec = word.vector\n",
    "        if str(word) in word2tfidf.keys():\n",
    "            # Step:3 Multiplying the numerical vector with its IDF\n",
    "            word_vec += (word2tfidf[str(word)] * vec)\n",
    "        if np.sum(word_vec) != 0:\n",
    "            word_cnt += 1\n",
    "    # Storing Avg IDF W2V for each sentences\n",
    "    q1_w2v.append(word_vec / word_cnt)\n",
    "\n",
    "df['q1_feats_m'] = list(q1_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [3:58:39<00:00, 28.23it/s]     \n"
     ]
    }
   ],
   "source": [
    "# Now for each row we need w2v \n",
    "\n",
    "# creating word to vec for question2\n",
    "q2_w2v = [] # this for each row\n",
    "\n",
    "#Step1. We need to loop thrrough each rows sentence\n",
    "for sentence in tqdm(df['question2'].values):\n",
    "    word_vec1 =  nlp(sentence)\n",
    "    # for each word we need to have empty vectors of the dimension (1, number of dimension trained)\n",
    "    word_vec = np.zeros(word_vec1.vector.shape)\n",
    "    word_cnt = 0\n",
    "    # Step:2 Looping through each sentence words\n",
    "    for word in word_vec1:\n",
    "        # Obtaining the polarity and numerical vector for each word\n",
    "        vec = word.vector\n",
    "        if str(word) in word2tfidf.keys():\n",
    "            # Step:3 Multiplying the numerical vector with its IDF\n",
    "            word_vec += (word2tfidf[str(word)] * vec)\n",
    "        if np.sum(word_vec) != 0:\n",
    "            word_cnt += 1\n",
    "    # Storing Avg IDF W2V for each sentences\n",
    "    q2_w2v.append(word_vec / word_cnt)\n",
    "\n",
    "df['q2_feats_m'] = list(q2_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepro_features_train.csv (Simple Preprocessing Feartures)\n",
    "#nlp_features_train.csv (NLP Features)\n",
    "if os.path.isfile('DataSets/nlp_and_fuzzy_train.csv'):\n",
    "    dfnlp = pd.read_csv(\"DataSets/nlp_and_fuzzy_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"run previous notebook -> 2_Quora_Preprocessing_new.ipynb\")\n",
    "\n",
    "if os.path.isfile('DataSets/df_feature_extraction.csv'):\n",
    "    dfppro = pd.read_csv(\"DataSets/df_feature_extraction.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"Run previous notebook -> 2_Quora_Preprocessing_new.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
    "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\n",
    "df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in nlp dataframe : 16\n",
      "Number of features in preprocessed dataframe : 12\n",
      "Number of features in question1 w2v  dataframe : 300\n",
      "Number of features in question2 w2v  dataframe : 300\n",
      "Number of features in final dataframe  : 628\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features in nlp dataframe :\", df1.shape[1])\n",
    "print(\"Number of features in preprocessed dataframe :\", df2.shape[1])\n",
    "print(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\n",
    "print(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\n",
    "print(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the final features to csv file\n",
    "if not os.path.isfile('DataSets/final_features_train.csv'):\n",
    "    df3_q1['id']=df1['id']\n",
    "    df3_q2['id']=df1['id']\n",
    "    df1  = df1.merge(df2, on='id',how='left')\n",
    "    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n",
    "    result  = df1.merge(df2, on='id',how='left')\n",
    "    result.to_csv('DataSets/final_features_train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
